{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 8: PySpark Optimization Lab"
      ],
      "metadata": {
        "id": "dnbalxqSpsCO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K2VDs4niUoR"
      },
      "source": [
        "## Objective\n",
        "\n",
        "In this lab, the goal is to improve the performance and memory efficiency of a PySpark job.\n",
        "We will do this by applying the following optimization techniques:\n",
        "\n",
        "- **Caching**: Store intermediate results in memory to avoid recomputation\n",
        "- **Repartitioning**: Distribute data more evenly across the cluster to enhance parallelism\n",
        "- **UDF Replacement**: Replace Python-based UDFs with native Spark built-in functions that execute faster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Summary\n",
        "\n",
        "Below is an overview of the dataset used in this lab.\n",
        "\n",
        "| Attribute                 | Description                                              |\n",
        "|--------------------------|----------------------------------------------------------|\n",
        "| **Filename**             | original_cleaned_nyc_taxi_data_2018.csv                 |\n",
        "| **Size**                 | ~750 MB                                                  |\n",
        "| **Format**               | CSV (Comma-Separated Values)                             |\n",
        "| **Number of Rows**       | ~5 million (depending on the full source)                |\n",
        "| **Columns**              | 21                                                       |\n",
        "| **Contains**             | NYC yellow taxi trip records for the year 2018          |\n",
        "| **Key Features**         | Trip distance, fare, tip, duration, pickup/dropoff IDs  |\n",
        "| **Time Dimensions**      | Year, Month, Day, Day of Week, Hour                      |\n",
        "| **Target Fields Used**   | tip_amount, fare_amount, trip_duration, trip_distance   |\n",
        "| **Use Case**             | Performance tuning via Spark DataFrame transformations  |"
      ],
      "metadata": {
        "id": "nslZ8SSQp9ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Steps\n",
        "\n",
        "This lab will consist of the following steps:\n",
        "\n",
        "1. **Load and inspect the dataset**  \n",
        "2. **Run an unoptimized pipeline**  \n",
        "   - Uses a Python UDF\n",
        "   - No caching or partitioning\n",
        "3. **Refactor into an optimized pipeline**  \n",
        "   - Replace UDF with built-in functions\n",
        "   - Apply caching\n",
        "   - Apply repartitioning\n",
        "4. **Benchmark performance**  \n",
        "   - Compare execution time between unoptimized and optimized jobs\n",
        "5. **Analyze execution plans**  \n",
        "   - Use `.explain(True)` to understand Spark's physical and logical execution strategies"
      ],
      "metadata": {
        "id": "jipY1FdpqAXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Setup\n",
        "\n",
        "We begin by importing required libraries and initializing a Spark session.  \n",
        "This session serves as the entry point for all PySpark operations.\n"
      ],
      "metadata": {
        "id": "4Hn9V_tPqGCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kqFC1Wm9iUoT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, expr\n",
        "\n",
        "# Create a Spark session (entry point to Spark functionality)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC Taxi Optimization Lab\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the gdown package to enable downloading from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "# Download the public dataset using its file ID from Google Drive\n",
        "!gdown --id 1p03CbxCZAahZN7eWSZ8QwCFz9DWxiedc --output original_cleaned_nyc_taxi_data_2018.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taI5b23jjUem",
        "outputId": "ffe226ae-3ede-4d9b-83be-85da00dd9b59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p03CbxCZAahZN7eWSZ8QwCFz9DWxiedc\n",
            "From (redirected): https://drive.google.com/uc?id=1p03CbxCZAahZN7eWSZ8QwCFz9DWxiedc&confirm=t&uuid=0a5c1c59-35b0-4a25-a747-e97bac8d2a80\n",
            "To: /content/original_cleaned_nyc_taxi_data_2018.csv\n",
            "100% 754M/754M [00:14<00:00, 51.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Inspect the Dataset\n",
        "We will now load the NYC Taxi dataset using `spark.read.csv`.  \n",
        "- The `header=True` option ensures that the first row is treated as column headers.  \n",
        "- The `inferSchema=True` option automatically detects and assigns appropriate data types.\n",
        "\n",
        "After loading, we will inspect the schema and a few sample rows."
      ],
      "metadata": {
        "id": "dOAmAn7lqSYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuRvJrJ1iUoU",
        "outputId": "dfce956a-8406-4ff3-c23e-fb5e7b8a7698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- rate_code: integer (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- payment_type: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- imp_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- pickup_location_id: integer (nullable = true)\n",
            " |-- dropoff_location_id: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- day_of_week: integer (nullable = true)\n",
            " |-- hour_of_day: integer (nullable = true)\n",
            " |-- trip_duration: double (nullable = true)\n",
            " |-- calculated_total_amount: double (nullable = true)\n",
            "\n",
            "+---+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+----+-----+---+-----------+-----------+-------------+-----------------------+\n",
            "|_c0|trip_distance|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|imp_surcharge|total_amount|pickup_location_id|dropoff_location_id|year|month|day|day_of_week|hour_of_day|trip_duration|calculated_total_amount|\n",
            "+---+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+----+-----+---+-----------+-----------+-------------+-----------------------+\n",
            "|  3|        16.97|        1|                 N|           1|       49.5|  0.0|    0.5|      5.61|        5.76|          0.3|       61.67|               231|                138|2018|    3| 29|          3|         13|       3317.0|     61.669999999999995|\n",
            "|  4|        14.45|        1|                 N|           1|       45.5|  0.0|    0.5|     10.41|        5.76|          0.3|       62.47|                87|                138|2018|    3| 29|          3|         14|       3648.0|      62.46999999999999|\n",
            "|  5|         11.6|        1|                 N|           1|       42.0|  0.0|    0.5|     14.57|        5.76|          0.3|       63.13|                68|                138|2018|    3| 29|          3|         14|       3540.0|     63.129999999999995|\n",
            "| 10|          5.1|        1|                 N|           1|       26.5|  1.0|    0.5|      5.65|         0.0|          0.3|       33.95|               186|                 33|2018|    3| 29|          3|         16|       2585.0|     33.949999999999996|\n",
            "| 12|        11.11|        1|                 N|           1|       45.5|  1.0|    0.5|     10.61|        5.76|          0.3|       63.67|               163|                138|2018|    3| 29|          3|         16|       4521.0|     63.669999999999995|\n",
            "+---+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+----+-----+---+-----------+-----------+-------------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file into a Spark DataFrame\n",
        "trips = spark.read.csv(\"original_cleaned_nyc_taxi_data_2018.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the schema to understand data types and column names\n",
        "trips.printSchema()\n",
        "\n",
        "# Show the first 5 rows to preview the data\n",
        "trips.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xha4OsQiUoU"
      },
      "source": [
        "## Unoptimized Pipeline\n",
        "\n",
        "In this step, we build an initial version of the Spark job **without any optimizations**.  \n",
        "Key characteristics of this unoptimized pipeline:\n",
        "\n",
        "- Uses a Python-based User Defined Function (UDF) to classify trips based on distance\n",
        "- No caching is applied, so all intermediate results are recomputed if reused\n",
        "- No partitioning is used, which may lead to skewed data distribution\n",
        "- Performs group-by aggregations on `pickup_location_id` and `trip_category`\n",
        "- Calculates:\n",
        "  - Average tip percentage\n",
        "  - Total fare revenue\n",
        "  - Average trip duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W5WCCvLsiUoU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import time\n",
        "\n",
        "# Define a Python UDF to classify trip distances into categories\n",
        "@udf(StringType())\n",
        "def trip_category(distance):\n",
        "    if distance is None:\n",
        "        return \"unknown\"\n",
        "    elif distance < 2:\n",
        "        return \"short\"\n",
        "    elif distance < 10:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"long\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start timer to measure unoptimized job runtime\n",
        "start_time = time.time()\n",
        "\n",
        "# Apply UDF, group by pickup location and trip category, and calculate aggregations\n",
        "df_unopt = trips.withColumn(\"trip_category\", trip_category(col(\"trip_distance\"))) \\\n",
        "    .groupBy(\"pickup_location_id\", \"trip_category\") \\\n",
        "    .agg(\n",
        "        expr(\"avg(tip_amount / fare_amount) AS avg_tip_pct\"),       # Average tip percentage\n",
        "        expr(\"sum(fare_amount) AS total_fare\"),                     # Total fare collected\n",
        "        expr(\"avg(trip_duration) AS avg_trip_duration\"),            # Average duration of trips\n",
        "        expr(\"count(*) AS num_trips\")                               # Number of trips in each group\n",
        "    )\n",
        "\n",
        "# Display the first 10 grouped results\n",
        "df_unopt.show(10, truncate=False)\n",
        "\n",
        "# Print the runtime for benchmarking\n",
        "print(f\"Unoptimized runtime: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REktMvhSoU3l",
        "outputId": "65aab416-34ff-4353-9f30-da13c467d1a9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------+--------------------+-------------------+------------------+---------+\n",
            "|pickup_location_id|trip_category|avg_tip_pct         |total_fare         |avg_trip_duration |num_trips|\n",
            "+------------------+-------------+--------------------+-------------------+------------------+---------+\n",
            "|164               |medium       |0.18146879695241228 |2407198.2800000003 |2204.711435696473 |92631    |\n",
            "|83                |medium       |0.15294876111007924 |19831.63           |2043.0618131868132|728      |\n",
            "|169               |medium       |0.06651791474826965 |4358.5599999999995 |2600.923076923077 |169      |\n",
            "|229               |short        |0.17797636285644572 |78406.2            |2080.5712018620397|9452     |\n",
            "|132               |long         |0.17016430811661032 |3.669964357999997E7|2211.642234573835 |730269   |\n",
            "|71                |medium       |0.06982431609781146 |11986.61           |2105.740909090909 |440      |\n",
            "|51                |short        |0.027346938775510202|253.74             |1823.0            |14       |\n",
            "|56                |short        |0.042709127892194255|375.63             |1638.9130434782608|23       |\n",
            "|245               |long         |0.1449302913693946  |661.76             |2232.909090909091 |11       |\n",
            "|246               |long         |0.18156529750708195 |713580.11          |2208.141013824885 |15190    |\n",
            "+------------------+-------------+--------------------+-------------------+------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Unoptimized runtime: 50.87 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oAdEukFiUoV"
      },
      "source": [
        "## Optimized Pipeline\n",
        "\n",
        "In this step, we refactor the Spark job to apply standard optimization techniques:\n",
        "\n",
        "- **Replaces the Python UDF** with a native Spark `when` / `otherwise` expression  \n",
        "  (which allows Spark to apply query optimization using Catalyst)\n",
        "- **Caches** the intermediate DataFrame to avoid redundant computation in future stages\n",
        "- **Repartitions** the data based on `pickup_location_id` to improve parallelism and reduce shuffle\n",
        "\n",
        "This version should be significantly faster and more memory-efficient compared to the unoptimized version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aR6vxgA-iUoV"
      },
      "outputs": [],
      "source": [
        "# Replace the UDF with Spark's native when/otherwise logic\n",
        "df_opt = trips.withColumn(\"trip_category\",\n",
        "    when(col(\"trip_distance\") < 2, \"short\")\n",
        "    .when(col(\"trip_distance\") < 10, \"medium\")\n",
        "    .otherwise(\"long\")\n",
        ")\n",
        "\n",
        "# Repartition data by pickup_location_id to improve parallel execution\n",
        "# Cache the result to avoid recomputation in future transformations\n",
        "df_opt = df_opt.repartition(\"pickup_location_id\").cache()\n",
        "\n",
        "# Trigger caching explicitly to load the data into memory\n",
        "df_opt.count()\n",
        "\n",
        "# Start timer to benchmark the optimized pipeline\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Optimized Aggregation and Benchmark\n",
        "\n",
        "Now we perform the same group-by and aggregation as before,  \n",
        "but on the optimized DataFrame that uses native functions, caching, and repartitioning.\n",
        "\n",
        "We measure the execution time to compare performance against the unoptimized job.\n",
        "\n",
        "This pipeline computes:\n",
        "- `avg_tip_pct`: Average tip as a percentage of fare\n",
        "- `total_fare`: Sum of fare amounts\n",
        "- `avg_trip_duration`: Average trip duration\n",
        "- `num_trips`: Total number of trips in each group"
      ],
      "metadata": {
        "id": "LTC2FjucrCR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform aggregations on the optimized DataFrame\n",
        "df_optimized = df_opt.groupBy(\"pickup_location_id\", \"trip_category\") \\\n",
        "    .agg(\n",
        "        expr(\"avg(tip_amount / fare_amount) AS avg_tip_pct\"),\n",
        "        expr(\"sum(fare_amount) AS total_fare\"),\n",
        "        expr(\"avg(trip_duration) AS avg_trip_duration\"),\n",
        "        expr(\"count(*) AS num_trips\")\n",
        "    )\n",
        "\n",
        "# Show first 10 result groups\n",
        "df_optimized.show(10, truncate=False)\n",
        "\n",
        "# Print runtime of optimized pipeline\n",
        "print(f\"Optimized runtime: {time.time() - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utVPJa3JoL6x",
        "outputId": "e83c5096-ee2f-4558-cff9-6daaf56bc134"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------------+---------------------+------------------+------------------+---------+\n",
            "|pickup_location_id|trip_category|avg_tip_pct          |total_fare        |avg_trip_duration |num_trips|\n",
            "+------------------+-------------+---------------------+------------------+------------------+---------+\n",
            "|148               |medium       |0.16581733792892117  |1701843.6600000006|2200.412935109583 |69810    |\n",
            "|148               |long         |0.21089685731788563  |688346.0600000005 |2207.885403954113 |16388    |\n",
            "|148               |short        |0.2695573443554674   |41056.01000000001 |2217.6414307658547|4557     |\n",
            "|243               |medium       |0.1704259153301238   |61693.76999999999 |2194.324134910206 |2283     |\n",
            "|243               |long         |0.14743155331239505  |41722.48000000001 |2210.81626187962  |947      |\n",
            "|243               |short        |0.12962486299747908  |2150.0299999999997|1919.111111111111 |171      |\n",
            "|31                |long         |0.1259056250569908   |4219.35           |1981.2333333333333|90       |\n",
            "|31                |medium       |0.1360482711153367   |979.5             |2998.4333333333334|30       |\n",
            "|31                |short        |0.0010344827586206897|34.5              |2362.0            |3        |\n",
            "|137               |short        |0.20434006343998784  |51138.53          |2188.3541769243393|6093     |\n",
            "+------------------+-------------+---------------------+------------------+------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Optimized runtime: 0.42 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZzXqxtKiUoW"
      },
      "source": [
        "## Plan Analysis\n",
        "\n",
        "To understand how Spark processes and optimizes each version of our job,  \n",
        "we examine the physical and logical execution plans generated by `.explain(True)`.\n",
        "\n",
        "This helps identify performance bottlenecks such as:\n",
        "- Unnecessary shuffling\n",
        "- Inefficient scans or wide transformations\n",
        "- Redundant computations due to missing caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZv1KmRViUoW",
        "outputId": "62e34673-b619-471b-d3f2-31126db5d5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UNOPTIMIZED PLAN ---\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['pickup_location_id, 'trip_category], ['pickup_location_id, 'trip_category, 'avg(('tip_amount / 'fare_amount)) AS avg_tip_pct#5415, 'sum('fare_amount) AS total_fare#5416, 'avg('trip_duration) AS avg_trip_duration#5417, 'count(1) AS num_trips#5418]\n",
            "+- Project [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, trip_category(trip_distance#5221)#5369 AS trip_category#5370]\n",
            "   +- Relation [_c0#5220,trip_distance#5221,rate_code#5222,store_and_fwd_flag#5223,payment_type#5224,fare_amount#5225,extra#5226,mta_tax#5227,tip_amount#5228,tolls_amount#5229,imp_surcharge#5230,total_amount#5231,pickup_location_id#5232,dropoff_location_id#5233,year#5234,month#5235,day#5236,day_of_week#5237,hour_of_day#5238,trip_duration#5239,calculated_total_amount#5240] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "pickup_location_id: int, trip_category: string, avg_tip_pct: double, total_fare: double, avg_trip_duration: double, num_trips: bigint\n",
            "Aggregate [pickup_location_id#5232, trip_category#5370], [pickup_location_id#5232, trip_category#5370, avg((tip_amount#5228 / fare_amount#5225)) AS avg_tip_pct#5415, sum(fare_amount#5225) AS total_fare#5416, avg(trip_duration#5239) AS avg_trip_duration#5417, count(1) AS num_trips#5418L]\n",
            "+- Project [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, trip_category(trip_distance#5221)#5369 AS trip_category#5370]\n",
            "   +- Relation [_c0#5220,trip_distance#5221,rate_code#5222,store_and_fwd_flag#5223,payment_type#5224,fare_amount#5225,extra#5226,mta_tax#5227,tip_amount#5228,tolls_amount#5229,imp_surcharge#5230,total_amount#5231,pickup_location_id#5232,dropoff_location_id#5233,year#5234,month#5235,day#5236,day_of_week#5237,hour_of_day#5238,trip_duration#5239,calculated_total_amount#5240] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [pickup_location_id#5232, trip_category#5370], [pickup_location_id#5232, trip_category#5370, avg((tip_amount#5228 / fare_amount#5225)) AS avg_tip_pct#5415, sum(fare_amount#5225) AS total_fare#5416, avg(trip_duration#5239) AS avg_trip_duration#5417, count(1) AS num_trips#5418L]\n",
            "+- Project [fare_amount#5225, tip_amount#5228, pickup_location_id#5232, trip_duration#5239, pythonUDF0#6595 AS trip_category#5370]\n",
            "   +- BatchEvalPython [trip_category(trip_distance#5221)#5369], [pythonUDF0#6595]\n",
            "      +- Project [trip_distance#5221, fare_amount#5225, tip_amount#5228, pickup_location_id#5232, trip_duration#5239]\n",
            "         +- Relation [_c0#5220,trip_distance#5221,rate_code#5222,store_and_fwd_flag#5223,payment_type#5224,fare_amount#5225,extra#5226,mta_tax#5227,tip_amount#5228,tolls_amount#5229,imp_surcharge#5230,total_amount#5231,pickup_location_id#5232,dropoff_location_id#5233,year#5234,month#5235,day#5236,day_of_week#5237,hour_of_day#5238,trip_duration#5239,calculated_total_amount#5240] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[pickup_location_id#5232, trip_category#5370], functions=[avg((tip_amount#5228 / fare_amount#5225)), sum(fare_amount#5225), avg(trip_duration#5239), count(1)], output=[pickup_location_id#5232, trip_category#5370, avg_tip_pct#5415, total_fare#5416, avg_trip_duration#5417, num_trips#5418L])\n",
            "   +- Exchange hashpartitioning(pickup_location_id#5232, trip_category#5370, 200), ENSURE_REQUIREMENTS, [plan_id=735]\n",
            "      +- HashAggregate(keys=[pickup_location_id#5232, trip_category#5370], functions=[partial_avg((tip_amount#5228 / fare_amount#5225)), partial_sum(fare_amount#5225), partial_avg(trip_duration#5239), partial_count(1)], output=[pickup_location_id#5232, trip_category#5370, sum#5454, count#5455L, sum#5456, sum#5457, count#5458L, count#5459L])\n",
            "         +- Project [fare_amount#5225, tip_amount#5228, pickup_location_id#5232, trip_duration#5239, pythonUDF0#6595 AS trip_category#5370]\n",
            "            +- BatchEvalPython [trip_category(trip_distance#5221)#5369], [pythonUDF0#6595]\n",
            "               +- FileScan csv [trip_distance#5221,fare_amount#5225,tip_amount#5228,pickup_location_id#5232,trip_duration#5239] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/original_cleaned_nyc_taxi_data_2018.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<trip_distance:double,fare_amount:double,tip_amount:double,pickup_location_id:int,trip_dura...\n",
            "\n",
            "--- OPTIMIZED PLAN ---\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['pickup_location_id, 'trip_category], ['pickup_location_id, 'trip_category, 'avg(('tip_amount / 'fare_amount)) AS avg_tip_pct#6104, 'sum('fare_amount) AS total_fare#6105, 'avg('trip_duration) AS avg_trip_duration#6106, 'count(1) AS num_trips#6107]\n",
            "+- RepartitionByExpression [pickup_location_id#5232]\n",
            "   +- Project [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, CASE WHEN (trip_distance#5221 < cast(2 as double)) THEN short WHEN (trip_distance#5221 < cast(10 as double)) THEN medium ELSE long END AS trip_category#5482]\n",
            "      +- Relation [_c0#5220,trip_distance#5221,rate_code#5222,store_and_fwd_flag#5223,payment_type#5224,fare_amount#5225,extra#5226,mta_tax#5227,tip_amount#5228,tolls_amount#5229,imp_surcharge#5230,total_amount#5231,pickup_location_id#5232,dropoff_location_id#5233,year#5234,month#5235,day#5236,day_of_week#5237,hour_of_day#5238,trip_duration#5239,calculated_total_amount#5240] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "pickup_location_id: int, trip_category: string, avg_tip_pct: double, total_fare: double, avg_trip_duration: double, num_trips: bigint\n",
            "Aggregate [pickup_location_id#5232, trip_category#5482], [pickup_location_id#5232, trip_category#5482, avg((tip_amount#5228 / fare_amount#5225)) AS avg_tip_pct#6104, sum(fare_amount#5225) AS total_fare#6105, avg(trip_duration#5239) AS avg_trip_duration#6106, count(1) AS num_trips#6107L]\n",
            "+- RepartitionByExpression [pickup_location_id#5232]\n",
            "   +- Project [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, CASE WHEN (trip_distance#5221 < cast(2 as double)) THEN short WHEN (trip_distance#5221 < cast(10 as double)) THEN medium ELSE long END AS trip_category#5482]\n",
            "      +- Relation [_c0#5220,trip_distance#5221,rate_code#5222,store_and_fwd_flag#5223,payment_type#5224,fare_amount#5225,extra#5226,mta_tax#5227,tip_amount#5228,tolls_amount#5229,imp_surcharge#5230,total_amount#5231,pickup_location_id#5232,dropoff_location_id#5233,year#5234,month#5235,day#5236,day_of_week#5237,hour_of_day#5238,trip_duration#5239,calculated_total_amount#5240] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [pickup_location_id#5232, trip_category#5482], [pickup_location_id#5232, trip_category#5482, avg((tip_amount#5228 / fare_amount#5225)) AS avg_tip_pct#6104, sum(fare_amount#5225) AS total_fare#6105, avg(trip_duration#5239) AS avg_trip_duration#6106, count(1) AS num_trips#6107L]\n",
            "+- Project [fare_amount#5225, tip_amount#5228, pickup_location_id#5232, trip_duration#5239, trip_category#5482]\n",
            "   +- InMemoryRelation [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, trip_category#5482], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "         +- AdaptiveSparkPlan isFinalPlan=false\n",
            "            +- Exchange hashpartitioning(pickup_location_id#29, 200), REPARTITION_BY_COL, [plan_id=99]\n",
            "               +- Project [_c0#17, trip_distance#18, rate_code#19, store_and_fwd_flag#20, payment_type#21, fare_amount#22, extra#23, mta_tax#24, tip_amount#25, tolls_amount#26, imp_surcharge#27, total_amount#28, pickup_location_id#29, dropoff_location_id#30, year#31, month#32, day#33, day_of_week#34, hour_of_day#35, trip_duration#36, calculated_total_amount#37, CASE WHEN (trip_distance#18 < 2.0) THEN short WHEN (trip_distance#18 < 10.0) THEN medium ELSE long END AS trip_category#279]\n",
            "                  +- FileScan csv [_c0#17,trip_distance#18,rate_code#19,store_and_fwd_flag#20,payment_type#21,fare_amount#22,extra#23,mta_tax#24,tip_amount#25,tolls_amount#26,imp_surcharge#27,total_amount#28,pickup_location_id#29,dropoff_location_id#30,year#31,month#32,day#33,day_of_week#34,hour_of_day#35,trip_duration#36,calculated_total_amount#37] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/original_cleaned_nyc_taxi_data_2018.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:int,trip_distance:double,rate_code:int,store_and_fwd_flag:string,payment_type:int,fare...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[pickup_location_id#5232, trip_category#5482], functions=[avg((tip_amount#5228 / fare_amount#5225)), sum(fare_amount#5225), avg(trip_duration#5239), count(1)], output=[pickup_location_id#5232, trip_category#5482, avg_tip_pct#6104, total_fare#6105, avg_trip_duration#6106, num_trips#6107L])\n",
            "   +- HashAggregate(keys=[pickup_location_id#5232, trip_category#5482], functions=[partial_avg((tip_amount#5228 / fare_amount#5225)), partial_sum(fare_amount#5225), partial_avg(trip_duration#5239), partial_count(1)], output=[pickup_location_id#5232, trip_category#5482, sum#6472, count#6473L, sum#6474, sum#6475, count#6476L, count#6477L])\n",
            "      +- InMemoryTableScan [fare_amount#5225, tip_amount#5228, pickup_location_id#5232, trip_duration#5239, trip_category#5482]\n",
            "            +- InMemoryRelation [_c0#5220, trip_distance#5221, rate_code#5222, store_and_fwd_flag#5223, payment_type#5224, fare_amount#5225, extra#5226, mta_tax#5227, tip_amount#5228, tolls_amount#5229, imp_surcharge#5230, total_amount#5231, pickup_location_id#5232, dropoff_location_id#5233, year#5234, month#5235, day#5236, day_of_week#5237, hour_of_day#5238, trip_duration#5239, calculated_total_amount#5240, trip_category#5482], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                  +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                     +- Exchange hashpartitioning(pickup_location_id#29, 200), REPARTITION_BY_COL, [plan_id=99]\n",
            "                        +- Project [_c0#17, trip_distance#18, rate_code#19, store_and_fwd_flag#20, payment_type#21, fare_amount#22, extra#23, mta_tax#24, tip_amount#25, tolls_amount#26, imp_surcharge#27, total_amount#28, pickup_location_id#29, dropoff_location_id#30, year#31, month#32, day#33, day_of_week#34, hour_of_day#35, trip_duration#36, calculated_total_amount#37, CASE WHEN (trip_distance#18 < 2.0) THEN short WHEN (trip_distance#18 < 10.0) THEN medium ELSE long END AS trip_category#279]\n",
            "                           +- FileScan csv [_c0#17,trip_distance#18,rate_code#19,store_and_fwd_flag#20,payment_type#21,fare_amount#22,extra#23,mta_tax#24,tip_amount#25,tolls_amount#26,imp_surcharge#27,total_amount#28,pickup_location_id#29,dropoff_location_id#30,year#31,month#32,day#33,day_of_week#34,hour_of_day#35,trip_duration#36,calculated_total_amount#37] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/original_cleaned_nyc_taxi_data_2018.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:int,trip_distance:double,rate_code:int,store_and_fwd_flag:string,payment_type:int,fare...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the execution plan for the unoptimized pipeline\n",
        "print(\"--- UNOPTIMIZED PLAN ---\")\n",
        "df_unopt.explain(True)\n",
        "\n",
        "# Print the execution plan for the optimized pipeline\n",
        "print(\"--- OPTIMIZED PLAN ---\")\n",
        "df_optimized.explain(True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}